{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/26aharikrishnan/Sentiment-Analysis-Project/blob/main/Sentiment_Analysis_of_Movie_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "3133ce81-f890-453f-a6b4-59f5b1c8653b",
      "metadata": {
        "id": "3133ce81-f890-453f-a6b4-59f5b1c8653b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83b3c74e-434b-4ae1-9e67-fe6831d10963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "tfidf = TfidfVectorizer(tokenizer=tokenize, min_df=5, max_df=0.8)\n",
        "tfidf.fit(X_train_texts)\n",
        "\n",
        "idf = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "f0275e5a-f8dc-4d26-992a-0a7ddcb5816d",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "f0275e5a-f8dc-4d26-992a-0a7ddcb5816d"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# Dataset\n",
        "# ----------------------\n",
        "\n",
        "# Load IMDB dataset safely\n",
        "df = pd.read_csv(\n",
        "    \"IMDB Dataset.csv\",\n",
        "    engine=\"python\",\n",
        "    on_bad_lines=\"skip\"\n",
        ")\n",
        "\n",
        "\n",
        "# Convert sentiment labels\n",
        "df[\"label\"] = df[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n",
        "\n",
        "texts = df[\"review\"].values\n",
        "labels = df[\"label\"].values\n",
        "\n",
        "X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
        "    texts,\n",
        "    labels,\n",
        "    test_size=0.30,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "04910fe5-90be-49e8-94da-0d8f33520fac",
      "metadata": {
        "id": "04910fe5-90be-49e8-94da-0d8f33520fac"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# Preprocessing\n",
        "# ----------------------\n",
        "\n",
        "def tokenize(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove HTML line breaks\n",
        "    text = re.sub(r\"<br\\s*/?>\", \" \", text)\n",
        "\n",
        "    # Remove punctuation and numbers\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Handle negation (e.g., \"not good\" → \"NOT_good\")\n",
        "    negation_words = {\"not\", \"no\", \"never\"}\n",
        "    result = []\n",
        "    negate = False\n",
        "\n",
        "    for word in tokens:\n",
        "        if word in negation_words:\n",
        "            negate = True\n",
        "            continue\n",
        "\n",
        "        if word in ENGLISH_STOP_WORDS:\n",
        "            continue\n",
        "\n",
        "        if negate:\n",
        "            result.append(\"NOT_\" + word)\n",
        "            negate = False\n",
        "        else:\n",
        "            result.append(word)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "32018bfe-2779-4809-9174-bc01aeba7504",
      "metadata": {
        "id": "32018bfe-2779-4809-9174-bc01aeba7504"
      },
      "outputs": [],
      "source": [
        "train_corpus = [tokenize(text) for text in X_train_texts]\n",
        "test_corpus = [tokenize(text) for text in X_test_texts]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "45d762f4-eb90-414f-be3c-0bc0527ba7eb",
      "metadata": {
        "id": "45d762f4-eb90-414f-be3c-0bc0527ba7eb"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# Train Word2Vec\n",
        "# ----------------------\n",
        "\n",
        "model = Word2Vec(\n",
        "    train_corpus[:15000],\n",
        "    vector_size=100,\n",
        "    window=8,\n",
        "    min_count=5,\n",
        "    sg=1,\n",
        "    negative=10,\n",
        "    epochs=5,\n",
        "    workers=8\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9a9b8c88-1283-413b-8e9d-325368aa116f",
      "metadata": {
        "id": "9a9b8c88-1283-413b-8e9d-325368aa116f"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# Sentence vector\n",
        "# ----------------------\n",
        "\n",
        "def sentence_vector(tokens):\n",
        "    vecs, weights = [], []\n",
        "\n",
        "    for w in tokens:\n",
        "        if w in model.wv and w in idf:\n",
        "            vecs.append(model.wv[w])\n",
        "            weights.append(idf[w])\n",
        "\n",
        "    if not vecs:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "    return np.average(vecs, axis=0, weights=weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "e696de6b-581a-4b13-ae0d-89f8e45ee774",
      "metadata": {
        "id": "e696de6b-581a-4b13-ae0d-89f8e45ee774"
      },
      "outputs": [],
      "source": [
        "def normalize(v):\n",
        "    return v / (np.linalg.norm(v) + 1e-9)\n",
        "\n",
        "X_train = np.array([normalize(sentence_vector(t)) for t in train_corpus])\n",
        "X_test = np.array([normalize(sentence_vector(t)) for t in test_corpus])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "6bf06419-4b6e-4123-93f6-39017a78f061",
      "metadata": {
        "id": "6bf06419-4b6e-4123-93f6-39017a78f061"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# Simple sentiment prototypes\n",
        "# ----------------------\n",
        "pos_vec = np.mean(X_train[y_train == 1], axis=0)\n",
        "neg_vec = np.mean(X_train[y_train == 0], axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "1b0de236-e4a2-4b83-ac78-64d402a1ac27",
      "metadata": {
        "id": "1b0de236-e4a2-4b83-ac78-64d402a1ac27"
      },
      "outputs": [],
      "source": [
        "def cosine(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "def predict(sentence):\n",
        "    tokens = tokenize(sentence)\n",
        "    v = sentence_vector(tokens)\n",
        "    return 1 if cosine(v, pos_vec) > cosine(v, neg_vec) else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "b07c3c4f-bdde-49b7-bce2-13856b3a07e8",
      "metadata": {
        "id": "b07c3c4f-bdde-49b7-bce2-13856b3a07e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f89ba090-3c3b-496d-b236-485bf08b771d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7665333333333333\n",
            "great acting and wonderful story → positive\n",
            "painfully slow and boring → negative\n",
            "not bad but not great → negative\n",
            "I loved the visuals → positive\n"
          ]
        }
      ],
      "source": [
        "# ----------------------\n",
        "# Try it\n",
        "# ----------------------\n",
        "\n",
        "predictions = [predict(text) for text in X_test_texts]\n",
        "\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "tests = [\n",
        "    \"great acting and wonderful story\",\n",
        "    \"painfully slow and boring\",\n",
        "    \"not bad but not great\",\n",
        "    \"I loved the visuals\"\n",
        "]\n",
        "\n",
        "for t in tests:\n",
        "    label = predict(t)\n",
        "    print(t, \"→\", \"positive\" if label == 1 else \"negative\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "efa81ad7-aadd-4e75-b281-658bf302f1d6",
      "metadata": {
        "id": "efa81ad7-aadd-4e75-b281-658bf302f1d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51b09844-9d97-4d6a-a985-08ec67726f97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "pip install gensim\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}